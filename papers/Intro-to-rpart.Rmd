---
title: "Partition Trees with `rpart`"
subtitle: "Stat 154, Fall 2017"
author: "Gaston Sanchez"
output: github_document
fontsize: 11pt
urlcolor: blue
---

> ### Learning Objectives:
>
> - Introduction to the `rpart()` function
> - Understand parameters
> - Understand control parameters

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(rpart)
```

------

## Decision Trees in R

The source code of the CART (Classification and Regression Trees) method, 
developed by Jerome Friedman, is proprietary software of Salford Systems.
However, a close implementation of CART in R is provided by the package 
`"tree"` and the more famous package `"rpart"`.

```{r eval = FALSE}
# install.packages("rpart")
library(rpart)
```

## Function `rpart()`

The main function of `"rpart"` is the homonym function `rpart()` which allows
you to build both classification and regression trees.

There are several ways in which you can use `rpart()`. So let's review its 
bells and whistles.

The three main arguments that you should pass to `rpart()` are: 

1. a `formula` indicating the response and the predictor variables
2. a `data` frame containing the variables
3. a `method` specifying whether a regression tree
(`method = "anova"`) or a classifiction tree (`method = "class"`) should be
computed.

For illustration purposes, let's see how to invoke `rpart()` to fit a 
classification tree for `Species` with the `iris` data set:

```{r}
# classification tree
tree0 <- rpart(Species ~ ., data = iris, method = "class")
```

`rpart()` returns an object of class `"rpart"` that, when printed, displays
some information about the obtained tree and its nodes:

```{r}
# rpart object
tree0
```

As you can tell from the output of `rpart()`, there are three main pieces of
information:

- The first line shows the total number of individuals in the root node: `n = 150`

- The second line shows the labels (legends) `node), split, n, loss, yval, (yprob)` 
to be used when reading the third part of the output:

- The third (and major) part of the output shows all the nodes in the tree;
starting with the first node which is the root node; the numbers inside the
parentheses correspond to the class proportions.

Tree-based models automatically select the more relevant variables. This means
that not all variable need to appear in the tree. In the previous example, 
only the variables `Petal.Length` and `Petal.Width` are used to fit the tree.

You read the tree output from the _root node_ that is marked with the number
`1)`. The `print()` method associated to an object `"rpart"` provides some
information of the data in this node. Namely, we can observe that we have 150
observations at this node, of which 100 have a different class from `setosa`,
and the corresponding relative frequencies (or probabilities) of each class.

Each node of a tree has two branches or partitions. For instance, from the 
root node we have a branch `2)` where the split criterion is 
`Petal.Length< 2.45`. If an observation has `Petal.Length< 2.45`, then it is
labeled as class `setosa`. In fact, all samples are of the same class. 
Likewise, from node `3)` the split condition is `Petal.Length>=2.45`, and 
two more nodes are derived from this partition: `6)` and `7)`.


### `plot()`ing a tree

In addition to the `print()` method of an object of class `"rpart"`, there is 
also a `plot()` method that produces a visual display of the tree. By the way,
when plotting an `"rpart"` tree you will also need to call `text()`:

```{r plot_rpart}
# basic (ugly) tree plot
plot(tree0)
text(tree0)
```

As you can tell, the output of `plot()` and `text()` tends to be kind of ugly.
`plot()` controls the graphical representation of the tree, the layout of the
nodes, their spacing, the form of the branches, and the margins of the graphic.
`text()`, on the other hand, adds labels and numeric information.

To get a prettier tree, you may want to tweak the `margin` of the plot figure,
and set to `TRUE` various parameters of `text()`:

```{r plot_rpart2}
# less basic tree plot
plot(tree0, margin = 0.15)
text(tree0, fancy = TRUE, use.n = TRUE, all = TRUE)
```

- `all = TRUE` labels all nodes
- `fancy = TRUE` shows internal nodes as ellipses, and the terminal nodes 
(i.e. the leafs) as rectangles
- `use.n = TRUE` displays the number of observations of each class

Here's another option to plot a tree:

```{r plot_rpart3}
# another less basic tree plot
plot(tree0, margin = 0.15, branch = 0.2)
text(tree0, fancy = TRUE, use.n = TRUE, all = TRUE)
```


In practice, trees are typically obtained in two steps. The first step involves
growing a large tree. Then, the second step involves pruning the tree by 
deleting bottom nodes through a process of penalized complexity. The main reason 
for this pruning process is to avoid overfitting. Why? Because overly large 
decision trees will fit the (training) data almost perfectly, but will also be
learning the unique noise associated to the data.

By default, `rpart()` grows a tree until some stopping critera are met. Namely,
the tree stops growing whenever: 1) the decrease in the purity goes below a 
certain threshold; when 2) the number of observations in the node is less than 
another threshold; or when 3) the tree depth exceeds a certain value.
These stopping criteria are controlled by the parameters `cp`, `minsplit`, 
and `maxdepth`.


### Parameters

Other parameters: `parms` and `control`, and `cp`

The argument `parms` takes a list of optional elements:

- `split` = the splitting index (or splitting criterion):
    + `"gini"` for the Gini index
    + `"information"` for the entropy
- `prior` = optional vector of prior probabilities
- `loss` = you can also specify a matrix of `loss` components

The argument `control` takes a list with:

- `minsplit` = minimum number of observations inside a node
- `minbucket` = minimum number of observations inside a node
- `cp` = complexity parameter
- `maxdepth` = maximum depth level
- _other parameters_ see (`?rpart.control`)

The argumet `cost` is a vector of non-negative costs, one for each variable in 
the model. Think of costs as scaling factors. By default, all variables have cost one.


### Cost Complexity

The `rpart()` function implements a pruning method called _cost complexity_
pruning. This method uses the values of the parameter `cp` that is calculated
for each node of the tree. The pruning method tries to estimate the value
of `cp` that ensures the best compromise between predictive accuracy and the
tree size.

Given an `"rpart"` tree, it is possible to obtain a set of sub-trees of this 
tree and estimate their predictive performance.

The argument `cp` refers to the _complexity parameter_

- a value of `cp = 0`, the tree is grown to its maximum depth
- a value of `cp > 0`, attempts to grow a tree of less depth


When fitting a classfication tree (`method = "class"`), the `rpart()` uses 
the Gini index as the splitting criterion.


### Method `summary()`

You can also use a `summary()` method on an `"rpart"` object. This will 
produce a large output plenty of information:

- The `CP` cost complexity 
- The _Variable importance_, normalized in order to have a total sum of 100
- Summaries for each node:
    + Predicted class, expected loss
    + class counts
    + The _Primary_ (chosen) splits
    + The _Surrogate_ splits

```{r}
# summary information
summary(tree0, digits = 3)
```


### Complexity and pruning

In addition to `summary()`, you can also use `printcp()` which allows you to
get more information about the complexity of the tree. More precisely, this 
function displays the so-called CP table:

```{r}
# CP table
printcp(tree0)
```

The CP table has five columns and as many rows as number of splits in the tree (the root node corresponds to `nsplit` = 0). 

- `CP` = complexity parameter
- `nsplit` = 
- `rel error` = relative error
- `xerror` = cross-validation error
- `xstd` = cross-validation standard deviation

The error rates are relative to the root node, this is why the `rel error` 
of the root node has a reference value of 1.
